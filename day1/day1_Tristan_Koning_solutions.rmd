---
title: 'Exercise_1 solution'
date: 'Sept. 16th, 2024'
author: 'Tristan Koning'
output: pdf_document
---

```{r}
library(ggplot2)
```

# Problem 1

## (a)
```{r}
# Setup
set.seed(1)
n <- 15
mu <- 4
std_dev_x <- 4
std_dev_err <- 2
beta_0 <- 1
beta_1 <- 2

# Sample x and build y
x <- rnorm(n = n, mean = mu, sd = std_dev_x^2)
y <- beta_0 + beta_1 * x + rnorm(n = n, mean = 0, sd = std_dev_err^2)

# Fit linear model
model <- lm(y ~ x)
summary(model)

# Plot data
df <- data.frame(x, y)
ggplot(data = df, aes(x = x, y = y)) +
  geom_point(size = 3) +
  geom_abline(intercept = beta_0, slope = beta_1, color = "red", size = 1.5) +
  geom_abline(intercept = coef(model)[1], slope = coef(model)[2], color = "blue", size = 1.5)
```

The estimated slope and intercept are `r coef(model)[1]` and `r coef(model)[2]`, while the true values are `r beta_0` and `r beta_1`, respectively. We can see that the estimated line is close to the true line.

## (b)
```{r}
# Setup
r <- 1000

simulation_df <- data.frame()

for (i in 1:r) {
  x_new <- sample(x = x, size = n, replace = TRUE)
  y_new <- beta_0 + beta_1 * x_new + rnorm(n = n, mean = 0, sd = std_dev_err^2)

  model <- lm(y_new ~ x_new)

  simulation_df <- rbind(simulation_df, c(coef(model)[1], coef(model)[2]))
}

colnames(simulation_df) <- c("intercept", "slope")

# Plot simulation results
par(mfrow = c(2, 2))
simulation_plot <- ggplot(data = df, aes(x = x, y = y)) +
  geom_point()

for (i in 1:nrow(simulation_df)) {
  simulation_plot <- simulation_plot + geom_abline(intercept = simulation_df$intercept[i], slope = simulation_df$slope[i], color = "blue", size = 0.5, linetype = "dashed")
}

simulation_plot + geom_abline(intercept = beta_0, slope = beta_1, color = "red", size = 1.5)

# Plot distribution of estimated intercept and slope
beta_0_plot <- ggplot(data = simulation_df, aes(x = intercept)) +
  geom_histogram(aes(y = ..density..), bins = 30) +
  ggtitle("Distribution of Estimated Intercept")

beta_0_plot + geom_vline(aes(xintercept = mean(intercept)), color = "red", size = 1.5)

beta_1_plot <- ggplot(data = simulation_df, aes(x = slope)) +
  geom_histogram(bins = 30) +
  ggtitle("Distribution of Estimated Slope")

beta_1_plot + geom_vline(aes(xintercept = mean(slope)), color = "red", size = 1.5)

# Summary of distribution
summary(simulation_df)
```

We can observe from the histograms as well as the mean and quartiles that the estimated values are very close to the true values (1 and 2, respectively).

## (c)
```{r}
# Setup
r <- 1000

resample_simulation_df <- data.frame()

for (i in 1:r) {
  x_new <- rnorm(n = n, mean = mu, sd = std_dev_x^2)
  y_new <- beta_0 + beta_1 * x_new + rnorm(n = n, mean = 0, sd = std_dev_err^2)

  model <- lm(y_new ~ x_new)

  resample_simulation_df <- rbind(resample_simulation_df, c(coef(model)[1], coef(model)[2]))
}

colnames(resample_simulation_df) <- c("intercept", "slope")

# Plot simulation results
resample_simulation_plot <- ggplot(data = df, aes(x = x, y = y)) +
  geom_point()

for (i in 1:nrow(resample_simulation_df)) {
  resample_simulation_plot <- resample_simulation_plot + geom_abline(intercept = resample_simulation_df$intercept[i], slope = resample_simulation_df$slope[i], color = "blue", size = 0.5, linetype = "dashed")
}

resample_simulation_plot + geom_abline(intercept = beta_0, slope = beta_1, color = "red", size = 1.5)

# Plot distribution of estimated intercept and slope
beta_0_plot <- ggplot(data = resample_simulation_df, aes(x = intercept)) +
  geom_histogram(aes(y = ..density..), bins = 30) +
  ggtitle("Distribution of Estimated Intercept")

beta_0_plot + geom_vline(aes(xintercept = mean(intercept)), color = "red", size = 1.5)

beta_1_plot <- ggplot(data = resample_simulation_df, aes(x = slope)) +
  geom_histogram(bins = 30) +
  ggtitle("Distribution of Estimated Slope")

beta_1_plot + geom_vline(aes(xintercept = mean(slope)), color = "red", size = 1.5)

# Summary of distribution
summary(resample_simulation_df)
```

In (b) we have applied a bootstrapping method while in (c) we have applied resampling. 
Looking at the graphs and the summary statistics, we notice that both methods yield results which are very close to the true parameters. 
However, Bootstrapping seems to be slightly more accurate.
Given that we have a large number of simulations (1000), this is not surprising. If it were a smaller number, bootstrapping would likely be a lot less accurate due to the small sample sizes per simulation (15).


# Problem 2

```{r}	
# Load data
require(fma)
bicoal
str(bicoal, strict.width = "cut")
year <- c(time(bicoal))
coal <- as.numeric(bicoal)
coal_df <- data.frame(coal = coal, year = year)
```

## (a)
```{r}
# Setup
set.seed(1)
rss <- data.frame(matrix(nrow = 100, ncol = 8))
par(mfrow = c(2, 4))

for (degree in 1:8) {
  # Plot points of data with x = year, y = coal and degree as title
  plot(year, coal, main = paste("Degree", degree), xlab = "Year", ylab = "Coal")

  for (i in 1:100) {
    training_indices <- sort(sample(49, 38))
    validation_indices <- setdiff(1:length(year), training_indices)

    model <- lm(coal ~ poly(year, degree), data = coal_df, subset = training_indices)
    prediction <- predict(model, newdata = coal_df[validation_indices, ])
    rss[i, degree] <- sum((prediction - coal_df$coal[-training_indices])^2)

    # Plot model
    lines(year[training_indices], model$fitted.values)
  }
}

rss_median <- apply(rss, 2, median)
which.min(rss_median)
rss_median[which.min(rss_median)]
```	

We can observe that a model with degree 7 has the lowest median RSS, therefore we choose this model.

(b)
```{r}
# Reserve first and last five years as a test set, training set = 26, validation set = 10
set.seed(1)
test_indices = c(1:5, 45:49)
rss <- data.frame(matrix(nrow = 100, ncol = 8))
par(mfrow = c(2, 4))

for (degree in 1:8) {
  # Plot points of data with x = year, y = coal and degree as title
  plot(year, coal, main = paste("Degree", degree), xlab = "Year", ylab = "Coal")

  for (i in 1:100) {
    training_indices <- sort(sample(40, 26))
    validation_indices <- setdiff(1:40, training_indices)

    model <- lm(coal ~ poly(year, degree), data = coal_df, subset = training_indices)
    prediction <- predict(model, newdata = coal_df[test_indices, ])
    rss[i, degree] <- sum((prediction - coal_df$coal[test_indices])^2)

    # Plot fitted line
    lines(year[training_indices], model$fitted.values)
  }
}

rss_median <- apply(rss, 2, median)
model_nr <- which.min(rss_median)

# Predict test set with best model
model <- lm(coal ~ poly(year, model_nr), data = coal_df, subset = test_indices)
prediction <- predict(model, newdata = coal_df[test_indices, ])

test_rss <- sum((prediction - coal_df$coal[test_indices])^2)
test_rss
prediction
coal_df[test_indices, ]
```	

The new optimal model has degree 1 according to the rss metric.
However, looking at the true values and the predicted values, we can see that the model is not very accurate and that the rss is quite high on the test set.
