---
title:  
    "Day5 exercise solutions"
date: 
    "Oct. 14th, 2024"
author:  
    "Ali Movasati"  
output:  
    pdf_document:
        latex_engine: xelatex
---


```{r global options}

# Set global code chunk options
knitr::opts_chunk$set(warning = FALSE)

```


```{r libraries}


# load required libraries
library(skimr)
library(ggplot2)
library(ggpubr)
library(magrittr)
library(dplyr)
library(tibble)


# define functions
`%notin%` <- Negate(`%in%`)


```

# Problem 1

```{r}

# read in the data

salary <- read.table(file = "/Users/alimos313/Documents/studies/phd/university/courses/stat-modelling/StatModelEx/day5/data/salary.txt", sep = ",", header = T)



```


## 1.A)


```{r}

salary %<>% mutate(size = factor(districtSize))


```


## 1.B) 


- Numerical summary

```{r}
# summary of dataset
skim(salary)


```

The average salary in USD is `r mean(salary$salary)` and in CHF is `r mean(salary$salary)*0.87`!


- Graphical summary

```{r}
# visualize the dataset


## scatter plot
salary %>% 
    ggplot(aes(x = experience, y = salary, color = size)) +
    geom_point(size = 3) +
    theme_bw()

## boxplots


bxp1 <- salary %>% 
    ggplot(aes(x = size, y = salary, color = size)) +
    geom_boxplot() +
    stat_compare_means(ref.group = "1", method = "t.test") +
    theme_bw() + 
    theme(legend.position = "none")

bxp2 <- salary %>% 
    ggplot(aes(x = size, y = experience, color = size)) +
    geom_boxplot() +
    stat_compare_means(ref.group = "1", method = "t.test") +
    theme_bw()

# Arrange the plots side by side
grid.arrange(bxp1, bxp2, ncol = 2)



```


## 1.C)

```{r}

# fit models

model_a <- lm(salary ~ 1 + experience + districtSize, data = salary)
model_b <- lm(salary ~ 1 + experience + size, data = salary)

## model A

summary(model_a)


## model A

summary(model_b)

```

### << comments >>

The two models are similar in terms of their goodness-of-fit values. Model A has one degree of freedom lower as it
treats districtSize as one independent variable, while model B treats factorized size variable as 2.


## 1.D)

```{r}

# save model summary in an object
summary_b <- summary(model_b)



```

### << comments >>

The adjusted coefficient of determination for model B is `r summary_b$adj.r.squared`. Therefore, `r summary_b$adj.r.squared*100`%
of variability in the response variable salary can be explained by the proposed linear regression model.

Given p-values < 0.05, it indicates that all predictors has a significant effect on the dependent variable. Therefore, it is not advisable to drop any of the predictor variables.

The parameter estimate for variable "experience" is `r summary_b$coefficients[2,"Estimate"]`. That means for 1 year additional experience, given the district is the same,
the salary will be increase by `r summary_b$coefficients[2,"Estimate"]`.

The parameter estimate for variable "district 2" is `r summary_b$coefficients[3,"Estimate"]`. That means for the same level of experience, teachers in district size 2, 
earn `r summary_b$coefficients[3,"Estimate"]` USD more than teachers in the reference district of size 1.

The parameter estimate for variable "district 3" is `r summary_b$coefficients[4,"Estimate"]`. That means for the same level of experience, teachers in district size 3, 
earn `r summary_b$coefficients[4,"Estimate"]` USD more than teachers in the reference district of size 1.




## 1.E)

```{r}

model_b_transformed <- lm(salary ~ I(experience - 13) + size, data = salary)

summary(model_b_transformed)

```

### << comments >>

When we modify the model by using "experience - 13" instead of experience, 
the interpretation of the coefficient for intercept and experience will change, but the overall fit and statistical properties of the model (likeR^2, adjusted R^2, p-values) will remain unchanged.

The coefficient for I(experience - 13) will now represent the change in salary for each year of experience compared to 13 years, 
and the intercept will reflect the average salary when the experience is 13 years, rather than 0 years.

Intercept: In the original model, the intercept represents the expected salary when experience = 0. 
This can be less meaningful if no teachers in the dataset have exactly 0 years of experience. 
By shifting the experience to center it around 13 years, the intercept will now represent the expected salary for a teacher with 13 years of experience.


## 1.F)

```{r}

# Plot diagnostic plots
par(mfrow = c(2, 2))  # Arrange 4 diagnostic plots
plot(model_b)


```



### << comments >>

- based on the residuals vs fitted plot we can confirm linear relationship between the response and predictor variables

- based on Q-Q Plot for Residuals we can confirm normality of error term of our linear regression model

- based on scale-location plot we can confirm homoscedasticity of the residuals across all levels of the independent variables


## 1.G)

```{r}

example_data_a <- data.frame(experience = c(10), districtSize = c(3))
example_data_b <- data.frame(experience = c(10), size = c("3"))


```

### << comments >>

According to model A the predicted salary will be `r predict(model_a, newdata = example_data_a)` and according to model B `r predict(model_b, newdata = example_data_b)`

---

# Problem 2

## 2.A)

we have the following formule:

SE(B^j) = sqrt(s**2.(XTX)^-1jj)

Therefore, we need the third element of the diagonal of the given matrix to compute SE which is `r sqrt()
```{r}

s2 <- 2

mat <- matrix(c(1,0.25,0.25,0.25,0.5,-0.25, 0.25, -0.25, 2), byrow = 3, ncol = 3)

```

Therefore, we need the third element of the diagonal of the given matrix to compute SE which is `r sqrt(s2*mat[3,3])`

## 2.B)

To test the hypothesis that 𝛽2=0 we will use a t-test for the regression coefficient 𝛽^2.

Hypotheses:
Null Hypothesis (𝐻0): 𝛽2=0 (there is no effect of the predictor 𝑥2)
Alternative Hypothesis (𝐻𝐴): 𝛽2≠0 (there is a significant effect of 𝑥2).

Test Statistic:

The test statistic for the hypothesis test is calculated as:

𝑡=𝛽^2/𝑆𝐸(𝛽^2)


```{r}

b2 <- 15
se_b2 <- sqrt(s2*mat[3,3])
t_statistic <- b2/se_b2

# degree of freedom

df <- 25 - 3


# get p.value

2 * pt(-7.5, df = 22)


```

since the p-value is smaller than 0.05, we reject the null hypothesis, concluding that 𝛽2≠0 and therefore 𝑥2 has a significant effect on y. 

## 2.C)
cov(𝛽1,𝛽2) is the (2,3) of covariance matrix times s^2, therefore it is `r mat[2,3]*s2`

```{r}

# formula for getting the SE of (B1-B2)

se_deduction <- sqrt(mat[2,2]*s2+mat[3,3]*s2-2*mat[2,3]*s2)

```

The standard error of βˆ1 − βˆ2 is `r se_deduction`

## 2.D)

To test the hypothesis that 𝛽1=𝛽2 we will use a t-test as follow:

Hypotheses:
Null Hypothesis (𝐻0): 𝛽1=𝛽2 
Alternative Hypothesis (𝐻𝐴): 𝛽1≠𝛽2

Test Statistic:

The test statistic for the hypothesis test is calculated as:

𝑡=(𝛽^1-𝛽^2)/𝑆𝐸(𝛽^1-𝛽^2)

```{r}

b1 <- 12
b2 <- 15
t_statistic <- (b1-b2)/se_deduction

2 * pt(-abs(t_statistic), df = 22)

```

The p-value is greater than 0.05, therefore we fail to reject the null hypothesis and conclude that there is no significant difference between 𝛽^1 and 𝛽^2

## 2.E)

```{r}

sst <- 120
sse <- s2*df

ssr <- sst - sse

```


SSR or the percentage of variation in y that is explained by the model is `r ssr/sst`%



```{r}

msr <- ssr/2
mse <- sse/22

f_statistic <- msr/mse

pf(f_statistic, df1 = 2, df2 = 22, lower.tail = FALSE)

```

The p-value is lower than 0.05 therefore the H0 (𝛽^1=𝛽^2=0) can be rejected. This means that the model with both predictors significantly explains the variation in the response variable y.